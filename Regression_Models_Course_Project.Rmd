---
title: 'Regression Models Course Project'
date: '19/09/2021'
author: 'Yulong Wang'
output:
  pdf_document: 
    latex_engine: xelatex
  html_document:
    keep_md: yes
---

```{r Folder}
setwd("/Users/yulong/GitHub/Statistical-Inference-Course-Project")
```

## Instructions

You work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:

* “Is an automatic or manual transmission better for MPG”

* "Quantify the MPG difference between automatic and manual transmissions"  

## Overview

To answer these questions, we conducted exploratory data analysis (EDA) and used hypothesis testing and linear regression. We have established simple and multiple linear regression analysis. However, the results of the multivariate regression model are more promising because it includes the potential effects of other variables on MPG.

Using the model selection strategy, we found that among all variables, weight and quarter-mile time (acceleration) have a significant effect on quantifying the difference in MPG between automatic and manual transmission cars.

## Data Processing

```{r Data, results="hide"}
library(datasets)
data(mtcars)
```

For the purpose of this analysis, we use the mtcars data set, which is a data set extracted from the American Automobile Trends magazine in 1974. It contains the fuel consumption of 32 cars (1973-74 models) and 10 of the car design and performance. Aspects. The following is a brief description of the variables in the data set:

It consists of 32 observations on 11 variables.

[, 1] mpg Miles/(US) gallon
[, 2] cyl Number of cylinders
[, 3] disp Displacement (cu.in.)
[, 4] hp Gross horsepower
[, 5] drat Rear axle ratio
[, 6] wt Weight (lb/1000)
[, 7] qsec 1/4 mile time
[, 8] vs V/S
[, 9] am Transmission (0 = automatic, 1 = manual)
[,10] gear Number of forward gears
[,11] carb Number of carburetors

```{r Str}
str(mtcars)
```

All the data is the num format, also, show the fisrt several rows of the data below:
```{r Show}
library(knitr)
library(printr)
kable(head(mtcars),align = 'c')
```

Please note that each row of mtcars represents a car model, which we can see in the row name. Each column is an attribute of the car, such as the number of miles per gallon (or fuel efficiency), the number of cylinders, the displacement (or volume) of the car's engine (in cubic inches), whether the car is an automatic transmission or a manual transmission, etc.

## Exploratory data analyses

```{r EDA}
library(GGally)
library(ggplot2)
ggpairs(mtcars, 
        upper = list(continuous = wrap("cor", size = 1.5)),
        lower = list(continuous = "smooth"),
        diag = list(continuous = "bar"),
        axisLabels='show')
```

It is also worthwhile check how MPG varies by automatic versus manual transmission. For that purpose we create a Violin plot of MPG by automatic and manual transmissions. In our dataset 0 represents an automatic transmission and 1 means a manual transmission.

```{r violon}
library(stats) 
ggplot(mtcars, aes(y=mpg, x=factor(am, labels = c("automatic", "manual")), fill=factor(am)))+
        geom_violin(colour="black", size=1)+
        xlab("transmission") + ylab("MPG")
```

We can form a clear hypothesis from this visualization: Compared with manual cars, autonomous cars seem to have lower miles per gallon and therefore lower fuel efficiency. But this obvious pattern may happen randomly—that is, we just happened to choose a group of inefficient automatic cars and a group of more efficient manual cars. Therefore, to check whether this is the case, we must use a statistical test.


## Model fitting and hypothesis testing

### Two samples t-test

We are interested to know if an automatic or manual transmission better for MPG. So first we test the hypothesis that cars with an automatic transmission use more fuel than cars with a manual transmission. To compare two samples to see if they have different means, we use two sample T-test.

```{r testing}
test <- t.test(mpg ~ am, data= mtcars, var.equal = FALSE, 
               paired=FALSE ,conf.level = .95)
result <- data.frame( "t-statistic"  = test$statistic, 
                       "df" = test$parameter,
                        "p-value"  = test$p.value,
                        "lower CL" = test$conf.int[1],
                        "upper CL" = test$conf.int[2],
                        "automatic mean" = test$estimate[1],
                        "manual mean" = test$estimate[2],
                        row.names = "")
kable(x = round(result,3),align = 'c')
```

p-value that shows the probability that this apparent difference between the two groups could appear by chance is very low. The confidence interval also describes how much lower the miles per gallon is in manual cars than it is in automatic cars. We can be confident that the true difference is between 3.2 and 11.3.

### Simple linear regression model

We can also fit factor variables as regressors and come up with thing like analysis of variance as a special case of linear regression models. From the “dummy variables” point of view, there’s nothing special about analysis of variance (ANOVA). It’s just linear regression in the special case that all predictor variables are categorical. Our factor variable in this case is Transmission (am).

```{r regression}
mtcars$amfactor <- factor(mtcars$am, labels = c("automatic", "manual")) 
summary(lm(mpg ~ factor(amfactor), data = mtcars))$coef
```

All the estimates provided here are in comparison with automatic transmission. The intercept of 17.14 is simply the mean MPG of automatic transmission. The slope of 7.24 is the change in the mean between manual transmission and automatic transmission. You can verify that from the plot as well. The p-value of 0.000285 for the mean MPG difference between manual and automatic transmission is significant. Therefore we conclude that according to this model manual transmission if more fuel efficient.

### Fitting multivariable linear regression model

Modeling based on only one predictor variable does not seem to be sufficient and good enough as we have other predictor variables that might affect MPG and therefore affect the difference in MPG by transmission. So the univariate model in this case is only part of the picture. Therefore in this part of the analysis we use multivariable linear regression to develop a model that includes the effect of other variables.

#### Model selection procedure

We want to know what combination of predictors will best predict fuel efficiency. Which predictors increase our accuracy by a statistically significant amount? We might be able to guess at the some of the trends from the graph, but really we want to perform a statistical test to determine which predictors are significant, and to determine the ideal formula for prediction.

Including variables that we should’t have increases actuall standard errors of the regression variables.Thus we don’t want to idly throw variables into the model. To confirm this fact, you can see below that if we include all the variables, not of them will a significant predictor of MPG (judging by p-value at the 95% confidence level).

```{r regression_cof}
summary(lm(mpg ~ cyl+disp+hp+drat+wt+qsec+factor(vs)+factor(am)+gear+carb, data = mtcars))$coef
```

#### Detecting collinearity

A major problem with multivariate regression is collinearity. If two or more predictor variables are highly correlated, and they are both entered into a regression model, it increases the true standard error and you get a very unstable estimates of the slope. We can assess the collinearity by variance inflation factor (VIF). Lets look at the variance inflation factors if we throw all the variables into the model.

```{r regression_vif}
library(car)
fitvif <- lm(mpg ~ cyl+disp+hp+drat+wt+qsec+factor(vs)+factor(am)+gear+carb, data = mtcars)
kable(vif(fitvif),align = 'c')
```

Values for the VIF that are greater than 10 are considered large. We should also pay attention to VIf values between 5 and 10. At these point we might consider leaving only one of these variables in the model.

#### Stepwise selection method

Among available methods we decided to perform stepwise selection to help us select a subset of variables that best explain the MPG. Please note that we also treat the vc variable as a categorical variable.

```{r regression_fit}
library(MASS)
fit <- lm(mpg ~ cyl+disp+hp+drat+wt+qsec+factor(vs)+factor(am)+gear+carb, data = mtcars)
step <- stepAIC(fit, direction="both", trace=FALSE)
summary(step)$coeff
```

```{r regression_sum}
summary(step)$r.squared
```


This shows that in addition to transmission, weight of the vehicle as well as acceleration speed have the highest relation to explaining the variation in mpg. The adjusted R^2 is 84% which means that the model explains 84% of the variation in mpg indicating it is a robust and highly predictive model.

#### Nested likelihood ratio test

If the models of interest are nested and without lots of parameters differentiating them, it’s fairly uncontroversial to use nested likelihood ratio tests. So in order to verify the result of the stepwise selection model, we also perform this procedure below.

```{r regression_nested likelihood}
fit1 <- lm(mpg ~ factor(am), data = mtcars)
fit2 <- lm(mpg ~ factor(am)+wt, data = mtcars)
fit3 <- lm(mpg ~ factor(am)+wt+qsec, data = mtcars)
fit4 <- lm(mpg ~ factor(am)+wt+qsec+hp, data = mtcars)
fit5 <- lm(mpg ~ factor(am)+wt+qsec+hp+drat, data = mtcars)
anova(fit1, fit2, fit3, fit4, fit5)

```

As you can see, the result is consistent with stepwise selection model and adding any more variable in addition to wt, am and qsec will dramatically increase the variation in the model, and the p-value immediately becomes insignificant.

#### Fitting the final model

Now using the selected variables, we can fit the final model.

```{r fitting final model}
finalfit <- lm(mpg ~ wt+qsec+factor(am), data = mtcars)
summary(finalfit)$coef
```

You can observe that all the variables now are statistically significant. This model explains 84% of the variance in miles per gallon (mpg). Now when we read the coefficient for am, we say that, on average, manual transmission cars have 2.94 MPGs more than automatic transmission cars. However this effect was much higher than when we did not adjust for weight and qsec.

## Regression diagnostics
In this section, we perform some diagnostics on the final regression model.

Detecting collinearity

This time looking at variance inflation factors reveal that the numbers are reasonable and we dont detect any sign of collinearity.

```{r collinearity}
fitvif <- lm(mpg ~ wt+qsec+factor(am), data = mtcars)
kable(vif(fitvif),align = 'c')
```

#### Residuals versus the fitted values

By plotting residuals versus the fitted values, we’re looking for any sort of pattern. Same thing with the fitted values versus the standardized, where it’s plotting a function of the standardized residuals. Plots below show that no specific pattern exist in the residuals.

```{r model1}
plot(finalfit, which=1)
```

```{r model3}
plot(finalfit, which=3)
```

#### Normality of residuals

The normal Q-Q plot, is you’re trying to figure out the normality of the errors by plotting the theoretical quantiles of the standard normal distribution by the standardized residuals.

```{r norm residuals}
qqPlot(finalfit, main="Normal Q-Q plot")
```


#### Influential Observations

Residuals versus leverage and also cooks distance, that’s where we want to look at the comparison of fit at that point verses the potential for influence of that point. So this is also a very useful plot to look at.


```{r model4}
plot(finalfit, which=4)
```



